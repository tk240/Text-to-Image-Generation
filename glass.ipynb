{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ae79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "import torch\n",
    "from pytorch_pretrained_biggan import BigGAN as DMBigGAN\n",
    "from pytorch_pretrained_biggan import  truncated_noise_sample, one_hot_from_names, display_in_terminal, save_as_images\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pytorch_pretrained_biggan import BigGAN\n",
    "import clip\n",
    "import kornia\n",
    "from PIL import Image\n",
    "from torchvision.utils import save_image\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.algorithms.so_genetic_algorithm import GA #so_genetic_algorithm\n",
    "from pymoo.factory import get_algorithm, get_decision_making, get_decomposition\n",
    "from pymoo.visualization.scatter import Scatter\n",
    "\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "from pymoo.factory import get_sampling, get_crossover, get_mutation\n",
    "from pymoo.operators.mixed_variable_operator import MixedVariableSampling, MixedVariableMutation, MixedVariableCrossover\n",
    "from pymoo.model.sampling import Sampling\n",
    "\n",
    "from pymoo.model.problem import Problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "783b81d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruncatedNormalRandomSampling(Sampling):\n",
    "    def __init__(self, var_type=np.float64):\n",
    "        super().__init__()\n",
    "        self.var_type = var_type\n",
    "\n",
    "    def _do(self, problem, n_samples, **kwargs):\n",
    "        return truncnorm.rvs(-2, 2, size=(n_samples, problem.n_var)).astype(np.float64)\n",
    "\n",
    "class NormalRandomSampling(Sampling):\n",
    "    def __init__(self, mu=0, std=1, var_type=np.float64):\n",
    "        super().__init__()\n",
    "        self.mu = mu\n",
    "        self.std = std\n",
    "        self.var_type = var_type\n",
    "\n",
    "    def _do(self, problem, n_samples, **kwargs):\n",
    "        return np.random.normal(self.mu, self.std, size=(n_samples, problem.n_var))\n",
    "\n",
    "class BinaryRandomSampling(Sampling):\n",
    "    def __init__(self, prob=0.5):\n",
    "        super().__init__()\n",
    "        self.prob = prob\n",
    "\n",
    "    def _do(self, problem, n_samples, **kwargs):\n",
    "        val = np.random.random((n_samples, problem.n_var))\n",
    "        return (val < self.prob).astype(np.bool_)\n",
    "\n",
    "def get_operators(config):\n",
    "    if config[\"config\"] == \"DeepMindBigGAN256\" or config[\"config\"] == \"DeepMindBigGAN512\":\n",
    "        mask = [\"real\"]*config[\"dim_z\"] + [\"bool\"]*config[\"num_classes\"]\n",
    "        \n",
    "        real_sampling = None\n",
    "        if config[\"config\"] == \"DeepMindBigGAN256\" or config[\"config\"] == \"DeepMindBigGAN512\":\n",
    "            real_sampling = TruncatedNormalRandomSampling()\n",
    "\n",
    "        sampling = MixedVariableSampling(mask, {\n",
    "            \"real\": real_sampling,\n",
    "            \"bool\": BinaryRandomSampling(prob=5/1000)\n",
    "        })\n",
    "\n",
    "        crossover = MixedVariableCrossover(mask, {\n",
    "            \"real\": get_crossover(\"real_sbx\", prob=1.0, eta=3.0),\n",
    "            \"bool\": get_crossover(\"bin_hux\", prob=0.2)\n",
    "        })\n",
    "\n",
    "        mutation = MixedVariableMutation(mask, {\n",
    "            \"real\": get_mutation(\"real_pm\", prob=0.5, eta=3.0),\n",
    "            \"bool\": get_mutation(\"bin_bitflip\", prob=10/1000)\n",
    "        })\n",
    "\n",
    "        return dict(\n",
    "            sampling=sampling,\n",
    "            crossover=crossover,\n",
    "            mutation=mutation\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8104ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_grid(images, path):\n",
    "    grid = torchvision.utils.make_grid(images)\n",
    "    torchvision.utils.save_image(grid, path)\n",
    "\n",
    "def show_grid(images):\n",
    "    grid = torchvision.utils.make_grid(images)\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().detach().numpy())\n",
    "    plt.show()\n",
    "\n",
    "def biggan_norm(images):\n",
    "    images = (images + 1) / 2.0\n",
    "    images = images.clip(0, 1)\n",
    "    return images\n",
    "\n",
    "def biggan_denorm(images):\n",
    "    images = images*2 - 1\n",
    "    return images\n",
    "\n",
    "\n",
    "def freeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f266bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepMindBigGAN(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(DeepMindBigGAN, self).__init__()\n",
    "        self.config = config\n",
    "        self.G = DMBigGAN.from_pretrained(config[\"weights\"])\n",
    "        self.D = None\n",
    "\n",
    "    def has_discriminator(self):\n",
    "        return False\n",
    "\n",
    "    def generate(self, z, class_labels, minibatch = None):\n",
    "        if minibatch is None:\n",
    "            return self.G(z, class_labels, self.config[\"truncation\"])\n",
    "        else:\n",
    "            assert z.shape[0] % minibatch == 0\n",
    "            gen_images = []\n",
    "            for i in range(0, z.shape[0] // minibatch):\n",
    "                z_minibatch = z[i*minibatch:(i+1)*minibatch, :]\n",
    "                cl_minibatch = class_labels[i*minibatch:(i+1)*minibatch, :]\n",
    "                gen_images.append(self.G(z_minibatch, cl_minibatch, self.config[\"truncation\"]))\n",
    "            gen_images = torch.cat(gen_images)\n",
    "            return gen_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e1fcc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerationProblem(Problem):\n",
    "    def __init__(self, config):\n",
    "        self.generator = Generator(config)\n",
    "        self.config = config\n",
    "\n",
    "        super().__init__(**self.config[\"problem_args\"])\n",
    "\n",
    "    def _evaluate(self, x, out, *args, **kwargs):\n",
    "        ls = self.config[\"latent\"](self.config)\n",
    "        ls.set_from_population(x)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated = self.generator.generate(ls, minibatch=self.config[\"batch_size\"])\n",
    "            sim = self.generator.clip_similarity(generated).cpu().numpy()\n",
    "            if self.config[\"problem_args\"][\"n_obj\"] == 2 and self.config[\"use_discriminator\"]:\n",
    "                dis = self.generator.discriminate(generated, minibatch=self.config[\"batch_size\"])\n",
    "                hinge = torch.relu(1 - dis)\n",
    "                hinge = hinge.squeeze(1).cpu().numpy()\n",
    "                out[\"F\"] = np.column_stack((-sim, hinge))\n",
    "            else:\n",
    "                out[\"F\"] = -sim\n",
    "\n",
    "            out[\"G\"] = np.zeros((x.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51d6e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepMindBigGANLatentSpace(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(DeepMindBigGANLatentSpace, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.z = torch.nn.Parameter(torch.tensor(truncated_noise_sample(self.config[\"batch_size\"])).to(self.config[\"device\"]))\n",
    "        self.class_labels = torch.nn.Parameter(torch.rand(self.config[\"batch_size\"], self.config[\"num_classes\"]).to(self.config[\"device\"]))\n",
    "    \n",
    "    def set_values(self, z, class_labels):\n",
    "        self.z.data = z\n",
    "        self.class_labels.data = class_labels\n",
    "\n",
    "    def set_from_population(self, x):\n",
    "        self.z.data = torch.tensor(x[:,:self.config[\"dim_z\"]].astype(float)).float().to(self.config[\"device\"])\n",
    "        self.class_labels.data = torch.tensor(x[:,self.config[\"dim_z\"]:].astype(float)).float().to(self.config[\"device\"])\n",
    "\n",
    "    def forward(self):\n",
    "        z = torch.clip(self.z, -2, 2)\n",
    "        class_labels = torch.softmax(self.class_labels, dim=1)\n",
    "\n",
    "        return z, class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17489c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = dict(\n",
    "    DeepMindBigGAN256 = dict(\n",
    "        task = \"txt2img\",\n",
    "        dim_z = 128,\n",
    "        num_classes = 1000,\n",
    "        latent = DeepMindBigGANLatentSpace,\n",
    "        model = DeepMindBigGAN,\n",
    "        weights = \"biggan-deep-256\",\n",
    "        use_discriminator = False,\n",
    "        algorithm = \"ga\",\n",
    "        norm = biggan_norm,\n",
    "        denorm = biggan_denorm,\n",
    "        truncation = 1.0,\n",
    "        pop_size = 64,\n",
    "        batch_size = 8,\n",
    "        problem_args = dict(\n",
    "            n_var = 128 + 1000,\n",
    "            n_obj = 1,\n",
    "            n_constr = 128,\n",
    "            xl = -2,\n",
    "            xu = 2\n",
    "        )\n",
    "    ),\n",
    "    DeepMindBigGAN512 = dict(\n",
    "        task = \"txt2img\",\n",
    "        dim_z = 128,\n",
    "        num_classes = 1000,\n",
    "        latent = DeepMindBigGANLatentSpace,\n",
    "        model = DeepMindBigGAN,\n",
    "        weights = \"biggan-deep-512\",\n",
    "        use_discriminator = False,\n",
    "        algorithm = \"ga\",\n",
    "        norm = biggan_norm,\n",
    "        denorm = biggan_denorm,\n",
    "        truncation = 1.0, #1.0\n",
    "        pop_size = 32, #32\n",
    "        batch_size = 1, #8\n",
    "        problem_args = dict(\n",
    "            n_var = 128 + 1000,\n",
    "            n_obj = 1,\n",
    "            n_constr = 128,\n",
    "            xl = -2,\n",
    "            xu = 2\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def get_config(name):\n",
    "    return configs[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edc293e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.augmentation = None\n",
    "\n",
    "        self.CLIP, clip_preprocess = clip.load(\"ViT-B/32\", device=self.config[\"device\"], jit=False)\n",
    "        \n",
    "        #Load the fine tuned clip model\n",
    "        checkpoint = torch.load('./models/model_fastai.pth')\n",
    "        self.CLIP.load_state_dict(checkpoint['model'])\n",
    "\n",
    "        self.CLIP = self.CLIP.eval()\n",
    "        freeze_model(self.CLIP)\n",
    "        self.model = self.config[\"model\"](config).to(self.config[\"device\"]).eval()\n",
    "        freeze_model(self.model)\n",
    "        \n",
    "        if config[\"task\"] == \"txt2img\":\n",
    "            self.tokens = clip.tokenize([self.config[\"target\"]]).to(self.config[\"device\"])\n",
    "            self.text_features = self.CLIP.encode_text(self.tokens).detach()\n",
    "        if config[\"task\"] == \"img2txt\":\n",
    "            image = clip_preprocess(Image.open(self.config[\"target\"])).unsqueeze(0).to(self.config[\"device\"])\n",
    "            self.image_features = self.CLIP.encode_image(image)\n",
    "\n",
    "    def generate(self, ls, minibatch=None):\n",
    "        z = ls()\n",
    "        result = self.model.generate(*z, minibatch=minibatch)\n",
    "        if hasattr(self.config, \"norm\"):\n",
    "            result = self.config[\"norm\"](result)\n",
    "        return result\n",
    "    \n",
    "    def discriminate(self, images, minibatch=None):\n",
    "        images = self.config[\"denorm\"](images)\n",
    "        return self.model.discriminate(images, minibatch)\n",
    "    \n",
    "    def has_discriminator(self):\n",
    "        return self.model.has_discriminator()\n",
    "\n",
    "    def clip_similarity(self, input):\n",
    "        if self.config[\"task\"] == \"txt2img\":\n",
    "            image = kornia.resize(input, (224, 224))\n",
    "            if self.augmentation is not None:\n",
    "                image = self.augmentation(image)\n",
    "\n",
    "            image_features = self.CLIP.encode_image(image)\n",
    "            \n",
    "            sim = torch.cosine_similarity(image_features, self.text_features)\n",
    "        elif self.config[\"task\"] == \"img2txt\":\n",
    "            try:\n",
    "                text_tokens = clip.tokenize(input).to(self.config[\"device\"])\n",
    "            except:\n",
    "                return torch.zeros(len(input))\n",
    "            text_features = self.CLIP.encode_text(text_tokens)\n",
    "\n",
    "            sim = torch.cosine_similarity(text_features, self.image_features)\n",
    "        return sim\n",
    "\n",
    "\n",
    "    def save(self, input, path):\n",
    "        if self.config[\"task\"] == \"txt2img\":\n",
    "            if input.shape[0] > 1:\n",
    "                save_grid(input.detach().cpu(), path)\n",
    "            else:\n",
    "                save_image(input[0], path)\n",
    "        elif self.config[\"task\"] == \"img2txt\":\n",
    "            f = open(path, \"w\")\n",
    "            f.write(\"\\n\".join(input))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aea43a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "        device = \"cuda\",\n",
    "        config = \"DeepMindBigGAN256\",\n",
    "        generations = 550,\n",
    "        save_each = 5,\n",
    "        tmp_folder = \"./tmp\",\n",
    "        target = \"a woman man and a dog standing in the snow\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3efa8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update(get_config(config[\"config\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75bc62b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'device': 'cuda',\n",
       " 'config': 'DeepMindBigGAN256',\n",
       " 'generations': 550,\n",
       " 'save_each': 5,\n",
       " 'tmp_folder': './tmp',\n",
       " 'target': 'a woman man and a dog standing in the snow',\n",
       " 'task': 'txt2img',\n",
       " 'dim_z': 128,\n",
       " 'num_classes': 1000,\n",
       " 'latent': __main__.DeepMindBigGANLatentSpace,\n",
       " 'model': __main__.DeepMindBigGAN,\n",
       " 'weights': 'biggan-deep-256',\n",
       " 'use_discriminator': False,\n",
       " 'algorithm': 'ga',\n",
       " 'norm': <function __main__.biggan_norm(images)>,\n",
       " 'denorm': <function __main__.biggan_denorm(images)>,\n",
       " 'truncation': 1.0,\n",
       " 'pop_size': 64,\n",
       " 'batch_size': 8,\n",
       " 'problem_args': {'n_var': 1128,\n",
       "  'n_obj': 1,\n",
       "  'n_constr': 128,\n",
       "  'xl': -2,\n",
       "  'xu': 2}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e76664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa842622",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "n_gen |  n_eval |   cv (min)   |   cv (avg)   |     fopt     |     favg    \n",
      "===========================================================================\n",
      "    1 |      64 |  0.00000E+00 |  0.00000E+00 |      -0.2156 |      -0.1327\n",
      "    2 |     128 |  0.00000E+00 |  0.00000E+00 |      -0.2666 |      -0.1653\n",
      "    3 |     192 |  0.00000E+00 |  0.00000E+00 |      -0.2666 |      -0.1844\n",
      "    4 |     256 |  0.00000E+00 |  0.00000E+00 |      -0.2666 |      -0.1958\n",
      "    5 |     320 |  0.00000E+00 |  0.00000E+00 |      -0.2754 |      -0.2079\n",
      "    6 |     384 |  0.00000E+00 |  0.00000E+00 |      -0.2754 |      -0.2134\n",
      "    7 |     448 |  0.00000E+00 |  0.00000E+00 |      -0.2754 |      -0.2201\n",
      "    8 |     512 |  0.00000E+00 |  0.00000E+00 |      -0.2754 |      -0.2263\n",
      "    9 |     576 |  0.00000E+00 |  0.00000E+00 |      -0.2754 |      -0.2301\n",
      "   10 |     640 |  0.00000E+00 |  0.00000E+00 |      -0.2844 |      -0.2361\n",
      "   11 |     704 |  0.00000E+00 |  0.00000E+00 |      -0.3213 |      -0.2434\n",
      "   12 |     768 |  0.00000E+00 |  0.00000E+00 |      -0.3213 |      -0.2473\n",
      "   13 |     832 |  0.00000E+00 |  0.00000E+00 |      -0.3213 |      -0.2505\n",
      "   14 |     896 |  0.00000E+00 |  0.00000E+00 |      -0.3213 |      -0.2559\n",
      "   15 |     960 |  0.00000E+00 |  0.00000E+00 |      -0.3213 |      -0.2612\n",
      "   16 |    1024 |  0.00000E+00 |  0.00000E+00 |      -0.3213 |      -0.2656\n",
      "   17 |    1088 |  0.00000E+00 |  0.00000E+00 |      -0.3213 |      -0.2683\n",
      "   18 |    1152 |  0.00000E+00 |  0.00000E+00 |      -0.3213 |       -0.271\n",
      "   19 |    1216 |  0.00000E+00 |  0.00000E+00 |      -0.3213 |      -0.2727\n",
      "   20 |    1280 |  0.00000E+00 |  0.00000E+00 |      -0.3213 |       -0.278\n",
      "   21 |    1344 |  0.00000E+00 |  0.00000E+00 |        -0.36 |       -0.282\n",
      "   22 |    1408 |  0.00000E+00 |  0.00000E+00 |        -0.36 |      -0.2837\n",
      "   23 |    1472 |  0.00000E+00 |  0.00000E+00 |        -0.36 |      -0.2844\n",
      "   24 |    1536 |  0.00000E+00 |  0.00000E+00 |        -0.36 |       -0.286\n",
      "   25 |    1600 |  0.00000E+00 |  0.00000E+00 |        -0.36 |      -0.2878\n",
      "   26 |    1664 |  0.00000E+00 |  0.00000E+00 |        -0.36 |      -0.2898\n",
      "   27 |    1728 |  0.00000E+00 |  0.00000E+00 |        -0.36 |       -0.291\n",
      "   28 |    1792 |  0.00000E+00 |  0.00000E+00 |        -0.36 |      -0.2927\n",
      "   29 |    1856 |  0.00000E+00 |  0.00000E+00 |        -0.36 |       -0.297\n",
      "   30 |    1920 |  0.00000E+00 |  0.00000E+00 |        -0.36 |         -0.3\n",
      "   31 |    1984 |  0.00000E+00 |  0.00000E+00 |        -0.36 |       -0.303\n",
      "   32 |    2048 |  0.00000E+00 |  0.00000E+00 |        -0.36 |      -0.3035\n",
      "   33 |    2112 |  0.00000E+00 |  0.00000E+00 |        -0.36 |      -0.3047\n",
      "   34 |    2176 |  0.00000E+00 |  0.00000E+00 |        -0.36 |       -0.307\n",
      "   35 |    2240 |  0.00000E+00 |  0.00000E+00 |        -0.36 |      -0.3098\n",
      "   36 |    2304 |  0.00000E+00 |  0.00000E+00 |        -0.36 |        -0.31\n",
      "   37 |    2368 |  0.00000E+00 |  0.00000E+00 |        -0.36 |      -0.3115\n",
      "   38 |    2432 |  0.00000E+00 |  0.00000E+00 |        -0.36 |       -0.315\n",
      "   39 |    2496 |  0.00000E+00 |  0.00000E+00 |      -0.3633 |      -0.3162\n",
      "   40 |    2560 |  0.00000E+00 |  0.00000E+00 |      -0.3633 |      -0.3179\n",
      "   41 |    2624 |  0.00000E+00 |  0.00000E+00 |      -0.3633 |      -0.3186\n",
      "   42 |    2688 |  0.00000E+00 |  0.00000E+00 |      -0.3633 |      -0.3198\n",
      "   43 |    2752 |  0.00000E+00 |  0.00000E+00 |      -0.3633 |      -0.3218\n",
      "   44 |    2816 |  0.00000E+00 |  0.00000E+00 |      -0.3633 |       -0.324\n",
      "   45 |    2880 |  0.00000E+00 |  0.00000E+00 |      -0.3633 |      -0.3252\n",
      "   46 |    2944 |  0.00000E+00 |  0.00000E+00 |      -0.3633 |      -0.3274\n",
      "   47 |    3008 |  0.00000E+00 |  0.00000E+00 |      -0.3633 |      -0.3303\n",
      "   48 |    3072 |  0.00000E+00 |  0.00000E+00 |      -0.3633 |       -0.332\n",
      "   49 |    3136 |  0.00000E+00 |  0.00000E+00 |      -0.3633 |      -0.3333\n",
      "   50 |    3200 |  0.00000E+00 |  0.00000E+00 |      -0.3633 |       -0.335\n",
      "   51 |    3264 |  0.00000E+00 |  0.00000E+00 |      -0.3633 |      -0.3364\n",
      "   52 |    3328 |  0.00000E+00 |  0.00000E+00 |      -0.3633 |      -0.3372\n",
      "   53 |    3392 |  0.00000E+00 |  0.00000E+00 |      -0.3633 |      -0.3386\n",
      "   54 |    3456 |  0.00000E+00 |  0.00000E+00 |      -0.3633 |      -0.3389\n",
      "   55 |    3520 |  0.00000E+00 |  0.00000E+00 |      -0.3633 |      -0.3396\n",
      "   56 |    3584 |  0.00000E+00 |  0.00000E+00 |      -0.3633 |       -0.341\n",
      "   57 |    3648 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3423\n",
      "   58 |    3712 |  0.00000E+00 |  0.00000E+00 |       -0.375 |       -0.344\n",
      "   59 |    3776 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3445\n",
      "   60 |    3840 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3455\n",
      "   61 |    3904 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3455\n",
      "   62 |    3968 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3457\n",
      "   63 |    4032 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3464\n",
      "   64 |    4096 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3477\n",
      "   65 |    4160 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3477\n",
      "   66 |    4224 |  0.00000E+00 |  0.00000E+00 |       -0.375 |       -0.348\n",
      "   67 |    4288 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3486\n",
      "   68 |    4352 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3486\n",
      "   69 |    4416 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3489\n",
      "   70 |    4480 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3494\n",
      "   71 |    4544 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3494\n",
      "   72 |    4608 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3494\n",
      "   73 |    4672 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3499\n",
      "   74 |    4736 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3503\n",
      "   75 |    4800 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3503\n",
      "   76 |    4864 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3506\n",
      "   77 |    4928 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3506\n",
      "   78 |    4992 |  0.00000E+00 |  0.00000E+00 |       -0.375 |       -0.351\n",
      "   79 |    5056 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3516\n",
      "   80 |    5120 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3518\n",
      "   81 |    5184 |  0.00000E+00 |  0.00000E+00 |       -0.375 |       -0.352\n",
      "   82 |    5248 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3523\n",
      "   83 |    5312 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3525\n",
      "   84 |    5376 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3525\n",
      "   85 |    5440 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3525\n",
      "   86 |    5504 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3525\n",
      "   87 |    5568 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3528\n",
      "   88 |    5632 |  0.00000E+00 |  0.00000E+00 |       -0.375 |       -0.353\n",
      "   89 |    5696 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3533\n",
      "   90 |    5760 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3533\n",
      "   91 |    5824 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3535\n",
      "   92 |    5888 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3538\n",
      "   93 |    5952 |  0.00000E+00 |  0.00000E+00 |       -0.375 |       -0.354\n",
      "   94 |    6016 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3542\n",
      "   95 |    6080 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3542\n",
      "   96 |    6144 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3542\n",
      "   97 |    6208 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3542\n",
      "   98 |    6272 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3547\n",
      "   99 |    6336 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3547\n",
      "  100 |    6400 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3552\n",
      "  101 |    6464 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3552\n",
      "  102 |    6528 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3552\n",
      "  103 |    6592 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3555\n",
      "  104 |    6656 |  0.00000E+00 |  0.00000E+00 |       -0.375 |       -0.356\n",
      "  105 |    6720 |  0.00000E+00 |  0.00000E+00 |       -0.375 |       -0.356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  106 |    6784 |  0.00000E+00 |  0.00000E+00 |       -0.375 |       -0.356\n",
      "  107 |    6848 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3564\n",
      "  108 |    6912 |  0.00000E+00 |  0.00000E+00 |       -0.375 |      -0.3564\n",
      "  109 |    6976 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |       -0.357\n",
      "  110 |    7040 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |      -0.3572\n",
      "  111 |    7104 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |       -0.358\n",
      "  112 |    7168 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |       -0.358\n",
      "  113 |    7232 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |       -0.358\n",
      "  114 |    7296 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |       -0.358\n",
      "  115 |    7360 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |       -0.358\n",
      "  116 |    7424 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |       -0.358\n",
      "  117 |    7488 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |      -0.3582\n",
      "  118 |    7552 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |      -0.3582\n",
      "  119 |    7616 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |      -0.3584\n",
      "  120 |    7680 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |      -0.3584\n",
      "  121 |    7744 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |      -0.3584\n",
      "  122 |    7808 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |      -0.3584\n",
      "  123 |    7872 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |      -0.3586\n",
      "  124 |    7936 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |      -0.3586\n",
      "  125 |    8000 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |      -0.3586\n",
      "  126 |    8064 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |       -0.359\n",
      "  127 |    8128 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |       -0.359\n",
      "  128 |    8192 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |       -0.359\n",
      "  129 |    8256 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |       -0.359\n",
      "  130 |    8320 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |      -0.3591\n",
      "  131 |    8384 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |      -0.3591\n",
      "  132 |    8448 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |      -0.3591\n",
      "  133 |    8512 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |      -0.3591\n",
      "  134 |    8576 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |      -0.3596\n",
      "  135 |    8640 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |      -0.3596\n",
      "  136 |    8704 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |      -0.3599\n",
      "  137 |    8768 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |      -0.3599\n",
      "  138 |    8832 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |      -0.3599\n",
      "  139 |    8896 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |      -0.3599\n",
      "  140 |    8960 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |      -0.3599\n",
      "  141 |    9024 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |      -0.3599\n",
      "  142 |    9088 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |      -0.3599\n",
      "  143 |    9152 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |        -0.36\n",
      "  144 |    9216 |  0.00000E+00 |  0.00000E+00 |      -0.3757 |        -0.36\n",
      "  145 |    9280 |  0.00000E+00 |  0.00000E+00 |      -0.3767 |      -0.3604\n",
      "  146 |    9344 |  0.00000E+00 |  0.00000E+00 |      -0.3767 |      -0.3604\n",
      "  147 |    9408 |  0.00000E+00 |  0.00000E+00 |      -0.3767 |      -0.3604\n",
      "  148 |    9472 |  0.00000E+00 |  0.00000E+00 |      -0.3767 |      -0.3604\n",
      "  149 |    9536 |  0.00000E+00 |  0.00000E+00 |      -0.3767 |      -0.3606\n",
      "  150 |    9600 |  0.00000E+00 |  0.00000E+00 |      -0.3767 |      -0.3606\n",
      "  151 |    9664 |  0.00000E+00 |  0.00000E+00 |      -0.3767 |      -0.3606\n",
      "  152 |    9728 |  0.00000E+00 |  0.00000E+00 |      -0.3767 |      -0.3608\n",
      "  153 |    9792 |  0.00000E+00 |  0.00000E+00 |      -0.3767 |      -0.3608\n",
      "  154 |    9856 |  0.00000E+00 |  0.00000E+00 |      -0.3767 |      -0.3608\n",
      "  155 |    9920 |  0.00000E+00 |  0.00000E+00 |      -0.3767 |      -0.3608\n",
      "  156 |    9984 |  0.00000E+00 |  0.00000E+00 |      -0.3767 |       -0.361\n",
      "  157 |   10048 |  0.00000E+00 |  0.00000E+00 |      -0.3767 |       -0.361\n",
      "  158 |   10112 |  0.00000E+00 |  0.00000E+00 |      -0.3767 |       -0.361\n",
      "  159 |   10176 |  0.00000E+00 |  0.00000E+00 |      -0.3767 |       -0.361\n",
      "  160 |   10240 |  0.00000E+00 |  0.00000E+00 |      -0.3767 |       -0.361\n",
      "  161 |   10304 |  0.00000E+00 |  0.00000E+00 |      -0.3767 |       -0.361\n",
      "  162 |   10368 |  0.00000E+00 |  0.00000E+00 |      -0.3767 |      -0.3613\n",
      "  163 |   10432 |  0.00000E+00 |  0.00000E+00 |      -0.3767 |      -0.3613\n",
      "  164 |   10496 |  0.00000E+00 |  0.00000E+00 |      -0.3767 |      -0.3613\n",
      "  165 |   10560 |  0.00000E+00 |  0.00000E+00 |      -0.3767 |      -0.3613\n",
      "  166 |   10624 |  0.00000E+00 |  0.00000E+00 |      -0.3767 |      -0.3613\n",
      "  167 |   10688 |  0.00000E+00 |  0.00000E+00 |      -0.3767 |      -0.3613\n",
      "  168 |   10752 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3618\n",
      "  169 |   10816 |  0.00000E+00 |  0.00000E+00 |       -0.382 |       -0.362\n",
      "  170 |   10880 |  0.00000E+00 |  0.00000E+00 |       -0.382 |       -0.362\n",
      "  171 |   10944 |  0.00000E+00 |  0.00000E+00 |       -0.382 |       -0.362\n",
      "  172 |   11008 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3623\n",
      "  173 |   11072 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3623\n",
      "  174 |   11136 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3623\n",
      "  175 |   11200 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3628\n",
      "  176 |   11264 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3628\n",
      "  177 |   11328 |  0.00000E+00 |  0.00000E+00 |       -0.382 |       -0.363\n",
      "  178 |   11392 |  0.00000E+00 |  0.00000E+00 |       -0.382 |       -0.363\n",
      "  179 |   11456 |  0.00000E+00 |  0.00000E+00 |       -0.382 |       -0.363\n",
      "  180 |   11520 |  0.00000E+00 |  0.00000E+00 |       -0.382 |       -0.363\n",
      "  181 |   11584 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3633\n",
      "  182 |   11648 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3633\n",
      "  183 |   11712 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3635\n",
      "  184 |   11776 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3635\n",
      "  185 |   11840 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3635\n",
      "  186 |   11904 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3635\n",
      "  187 |   11968 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3635\n",
      "  188 |   12032 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3635\n",
      "  189 |   12096 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3635\n",
      "  190 |   12160 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3638\n",
      "  191 |   12224 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3638\n",
      "  192 |   12288 |  0.00000E+00 |  0.00000E+00 |       -0.382 |       -0.364\n",
      "  193 |   12352 |  0.00000E+00 |  0.00000E+00 |       -0.382 |       -0.364\n",
      "  194 |   12416 |  0.00000E+00 |  0.00000E+00 |       -0.382 |       -0.364\n",
      "  195 |   12480 |  0.00000E+00 |  0.00000E+00 |       -0.382 |       -0.364\n",
      "  196 |   12544 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3643\n",
      "  197 |   12608 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3643\n",
      "  198 |   12672 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3645\n",
      "  199 |   12736 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3645\n",
      "  200 |   12800 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3645\n",
      "  201 |   12864 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3645\n",
      "  202 |   12928 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3645\n",
      "  203 |   12992 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3645\n",
      "  204 |   13056 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3647\n",
      "  205 |   13120 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3647\n",
      "  206 |   13184 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3647\n",
      "  207 |   13248 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3647\n",
      "  208 |   13312 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3647\n",
      "  209 |   13376 |  0.00000E+00 |  0.00000E+00 |       -0.382 |       -0.365\n",
      "  210 |   13440 |  0.00000E+00 |  0.00000E+00 |       -0.382 |       -0.365\n",
      "  211 |   13504 |  0.00000E+00 |  0.00000E+00 |       -0.382 |       -0.365\n",
      "  212 |   13568 |  0.00000E+00 |  0.00000E+00 |       -0.382 |       -0.365\n",
      "  213 |   13632 |  0.00000E+00 |  0.00000E+00 |       -0.382 |       -0.365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  214 |   13696 |  0.00000E+00 |  0.00000E+00 |       -0.382 |       -0.365\n",
      "  215 |   13760 |  0.00000E+00 |  0.00000E+00 |       -0.382 |       -0.365\n",
      "  216 |   13824 |  0.00000E+00 |  0.00000E+00 |       -0.382 |       -0.365\n",
      "  217 |   13888 |  0.00000E+00 |  0.00000E+00 |       -0.382 |       -0.365\n",
      "  218 |   13952 |  0.00000E+00 |  0.00000E+00 |       -0.382 |       -0.365\n",
      "  219 |   14016 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3652\n",
      "  220 |   14080 |  0.00000E+00 |  0.00000E+00 |       -0.382 |      -0.3652\n",
      "  221 |   14144 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3655\n",
      "  222 |   14208 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3657\n",
      "  223 |   14272 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3657\n",
      "  224 |   14336 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3657\n",
      "  225 |   14400 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.366\n",
      "  226 |   14464 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.366\n",
      "  227 |   14528 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.366\n",
      "  228 |   14592 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.366\n",
      "  229 |   14656 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.366\n",
      "  230 |   14720 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.366\n",
      "  231 |   14784 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3665\n",
      "  232 |   14848 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3665\n",
      "  233 |   14912 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3665\n",
      "  234 |   14976 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3665\n",
      "  235 |   15040 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3665\n",
      "  236 |   15104 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3665\n",
      "  237 |   15168 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3665\n",
      "  238 |   15232 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3667\n",
      "  239 |   15296 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.367\n",
      "  240 |   15360 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.367\n",
      "  241 |   15424 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.367\n",
      "  242 |   15488 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.367\n",
      "  243 |   15552 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3672\n",
      "  244 |   15616 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3672\n",
      "  245 |   15680 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3672\n",
      "  246 |   15744 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3672\n",
      "  247 |   15808 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3672\n",
      "  248 |   15872 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3672\n",
      "  249 |   15936 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3672\n",
      "  250 |   16000 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3672\n",
      "  251 |   16064 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3672\n",
      "  252 |   16128 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3674\n",
      "  253 |   16192 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3674\n",
      "  254 |   16256 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3674\n",
      "  255 |   16320 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3674\n",
      "  256 |   16384 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3674\n",
      "  257 |   16448 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3674\n",
      "  258 |   16512 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3674\n",
      "  259 |   16576 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3674\n",
      "  260 |   16640 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3677\n",
      "  261 |   16704 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3677\n",
      "  262 |   16768 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3677\n",
      "  263 |   16832 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3677\n",
      "  264 |   16896 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.368\n",
      "  265 |   16960 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.368\n",
      "  266 |   17024 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.368\n",
      "  267 |   17088 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.368\n",
      "  268 |   17152 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3682\n",
      "  269 |   17216 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3682\n",
      "  270 |   17280 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3682\n",
      "  271 |   17344 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3682\n",
      "  272 |   17408 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3682\n",
      "  273 |   17472 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3682\n",
      "  274 |   17536 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3682\n",
      "  275 |   17600 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3682\n",
      "  276 |   17664 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3682\n",
      "  277 |   17728 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3682\n",
      "  278 |   17792 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3682\n",
      "  279 |   17856 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3682\n",
      "  280 |   17920 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3682\n",
      "  281 |   17984 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3682\n",
      "  282 |   18048 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3684\n",
      "  283 |   18112 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3684\n",
      "  284 |   18176 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3684\n",
      "  285 |   18240 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3687\n",
      "  286 |   18304 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3687\n",
      "  287 |   18368 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3687\n",
      "  288 |   18432 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3687\n",
      "  289 |   18496 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3687\n",
      "  290 |   18560 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3687\n",
      "  291 |   18624 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3687\n",
      "  292 |   18688 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3687\n",
      "  293 |   18752 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3687\n",
      "  294 |   18816 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3687\n",
      "  295 |   18880 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3687\n",
      "  296 |   18944 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3687\n",
      "  297 |   19008 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3687\n",
      "  298 |   19072 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3687\n",
      "  299 |   19136 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.369\n",
      "  300 |   19200 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.369\n",
      "  301 |   19264 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.369\n",
      "  302 |   19328 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.369\n",
      "  303 |   19392 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.369\n",
      "  304 |   19456 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.369\n",
      "  305 |   19520 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.369\n",
      "  306 |   19584 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.369\n",
      "  307 |   19648 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.369\n",
      "  308 |   19712 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.369\n",
      "  309 |   19776 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.369\n",
      "  310 |   19840 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.369\n",
      "  311 |   19904 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.369\n",
      "  312 |   19968 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.369\n",
      "  313 |   20032 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3696\n",
      "  314 |   20096 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3696\n",
      "  315 |   20160 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3696\n",
      "  316 |   20224 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3696\n",
      "  317 |   20288 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3696\n",
      "  318 |   20352 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3699\n",
      "  319 |   20416 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3699\n",
      "  320 |   20480 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3699\n",
      "  321 |   20544 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  322 |   20608 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3699\n",
      "  323 |   20672 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3699\n",
      "  324 |   20736 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3699\n",
      "  325 |   20800 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3699\n",
      "  326 |   20864 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3699\n",
      "  327 |   20928 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3699\n",
      "  328 |   20992 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3699\n",
      "  329 |   21056 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3699\n",
      "  330 |   21120 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3699\n",
      "  331 |   21184 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3699\n",
      "  332 |   21248 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3699\n",
      "  333 |   21312 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3699\n",
      "  334 |   21376 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3704\n",
      "  335 |   21440 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3704\n",
      "  336 |   21504 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3704\n",
      "  337 |   21568 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3704\n",
      "  338 |   21632 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3704\n",
      "  339 |   21696 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3704\n",
      "  340 |   21760 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3704\n",
      "  341 |   21824 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3706\n",
      "  342 |   21888 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3706\n",
      "  343 |   21952 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3706\n",
      "  344 |   22016 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3706\n",
      "  345 |   22080 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3706\n",
      "  346 |   22144 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3706\n",
      "  347 |   22208 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3706\n",
      "  348 |   22272 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3708\n",
      "  349 |   22336 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3708\n",
      "  350 |   22400 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3708\n",
      "  351 |   22464 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.371\n",
      "  352 |   22528 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.371\n",
      "  353 |   22592 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.371\n",
      "  354 |   22656 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.371\n",
      "  355 |   22720 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.371\n",
      "  356 |   22784 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.371\n",
      "  357 |   22848 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.371\n",
      "  358 |   22912 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.371\n",
      "  359 |   22976 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.371\n",
      "  360 |   23040 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.371\n",
      "  361 |   23104 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.371\n",
      "  362 |   23168 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.371\n",
      "  363 |   23232 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3713\n",
      "  364 |   23296 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3713\n",
      "  365 |   23360 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3713\n",
      "  366 |   23424 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3713\n",
      "  367 |   23488 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3713\n",
      "  368 |   23552 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3713\n",
      "  369 |   23616 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3713\n",
      "  370 |   23680 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3713\n",
      "  371 |   23744 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3713\n",
      "  372 |   23808 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3713\n",
      "  373 |   23872 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3713\n",
      "  374 |   23936 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3713\n",
      "  375 |   24000 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3716\n",
      "  376 |   24064 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3716\n",
      "  377 |   24128 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3716\n",
      "  378 |   24192 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3716\n",
      "  379 |   24256 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3716\n",
      "  380 |   24320 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3716\n",
      "  381 |   24384 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3716\n",
      "  382 |   24448 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3716\n",
      "  383 |   24512 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3716\n",
      "  384 |   24576 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3716\n",
      "  385 |   24640 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3716\n",
      "  386 |   24704 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3716\n",
      "  387 |   24768 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3716\n",
      "  388 |   24832 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3716\n",
      "  389 |   24896 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3716\n",
      "  390 |   24960 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3716\n",
      "  391 |   25024 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3716\n",
      "  392 |   25088 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3718\n",
      "  393 |   25152 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3718\n",
      "  394 |   25216 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3718\n",
      "  395 |   25280 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3718\n",
      "  396 |   25344 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3718\n",
      "  397 |   25408 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3718\n",
      "  398 |   25472 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3718\n",
      "  399 |   25536 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.372\n",
      "  400 |   25600 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.372\n",
      "  401 |   25664 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.372\n",
      "  402 |   25728 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.372\n",
      "  403 |   25792 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.372\n",
      "  404 |   25856 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.372\n",
      "  405 |   25920 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.372\n",
      "  406 |   25984 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.372\n",
      "  407 |   26048 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.372\n",
      "  408 |   26112 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.372\n",
      "  409 |   26176 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.372\n",
      "  410 |   26240 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.372\n",
      "  411 |   26304 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.372\n",
      "  412 |   26368 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.372\n",
      "  413 |   26432 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.372\n",
      "  414 |   26496 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.372\n",
      "  415 |   26560 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.372\n",
      "  416 |   26624 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.372\n",
      "  417 |   26688 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.372\n",
      "  418 |   26752 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.372\n",
      "  419 |   26816 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.372\n",
      "  420 |   26880 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.372\n",
      "  421 |   26944 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.372\n",
      "  422 |   27008 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3723\n",
      "  423 |   27072 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3723\n",
      "  424 |   27136 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3723\n",
      "  425 |   27200 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3723\n",
      "  426 |   27264 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3723\n",
      "  427 |   27328 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3723\n",
      "  428 |   27392 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3723\n",
      "  429 |   27456 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  430 |   27520 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3723\n",
      "  431 |   27584 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3723\n",
      "  432 |   27648 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3723\n",
      "  433 |   27712 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3723\n",
      "  434 |   27776 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3723\n",
      "  435 |   27840 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3723\n",
      "  436 |   27904 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3723\n",
      "  437 |   27968 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3723\n",
      "  438 |   28032 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3723\n",
      "  439 |   28096 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3723\n",
      "  440 |   28160 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3723\n",
      "  441 |   28224 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3723\n",
      "  442 |   28288 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3723\n",
      "  443 |   28352 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3723\n",
      "  444 |   28416 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3723\n",
      "  445 |   28480 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3726\n",
      "  446 |   28544 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3726\n",
      "  447 |   28608 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3726\n",
      "  448 |   28672 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3726\n",
      "  449 |   28736 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3726\n",
      "  450 |   28800 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3726\n",
      "  451 |   28864 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3726\n",
      "  452 |   28928 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3726\n",
      "  453 |   28992 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3726\n",
      "  454 |   29056 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3726\n",
      "  455 |   29120 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3726\n",
      "  456 |   29184 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3726\n",
      "  457 |   29248 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3726\n",
      "  458 |   29312 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3726\n",
      "  459 |   29376 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3728\n",
      "  460 |   29440 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3728\n",
      "  461 |   29504 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3728\n",
      "  462 |   29568 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3728\n",
      "  463 |   29632 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3728\n",
      "  464 |   29696 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3728\n",
      "  465 |   29760 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3728\n",
      "  466 |   29824 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3728\n",
      "  467 |   29888 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3728\n",
      "  468 |   29952 |  0.00000E+00 |  0.00000E+00 |       -0.385 |      -0.3728\n",
      "  469 |   30016 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.373\n",
      "  470 |   30080 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.373\n",
      "  471 |   30144 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.373\n",
      "  472 |   30208 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.373\n",
      "  473 |   30272 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.373\n",
      "  474 |   30336 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.373\n",
      "  475 |   30400 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.373\n",
      "  476 |   30464 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.373\n",
      "  477 |   30528 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.373\n",
      "  478 |   30592 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.373\n",
      "  479 |   30656 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.373\n",
      "  480 |   30720 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.373\n",
      "  481 |   30784 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.373\n",
      "  482 |   30848 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.373\n",
      "  483 |   30912 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.373\n",
      "  484 |   30976 |  0.00000E+00 |  0.00000E+00 |       -0.385 |       -0.373\n",
      "  485 |   31040 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3735\n",
      "  486 |   31104 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3735\n",
      "  487 |   31168 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3735\n",
      "  488 |   31232 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3735\n",
      "  489 |   31296 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3738\n",
      "  490 |   31360 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3738\n",
      "  491 |   31424 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3738\n",
      "  492 |   31488 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3738\n",
      "  493 |   31552 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3738\n",
      "  494 |   31616 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3738\n",
      "  495 |   31680 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3738\n",
      "  496 |   31744 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3738\n",
      "  497 |   31808 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |       -0.374\n",
      "  498 |   31872 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |       -0.374\n",
      "  499 |   31936 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |       -0.374\n",
      "  500 |   32000 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |       -0.374\n",
      "  501 |   32064 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |       -0.374\n",
      "  502 |   32128 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |       -0.374\n",
      "  503 |   32192 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |       -0.374\n",
      "  504 |   32256 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |       -0.374\n",
      "  505 |   32320 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |       -0.374\n",
      "  506 |   32384 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |       -0.374\n",
      "  507 |   32448 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |       -0.374\n",
      "  508 |   32512 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |       -0.374\n",
      "  509 |   32576 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |       -0.374\n",
      "  510 |   32640 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |       -0.374\n",
      "  511 |   32704 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |       -0.374\n",
      "  512 |   32768 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |       -0.374\n",
      "  513 |   32832 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |       -0.374\n",
      "  514 |   32896 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3743\n",
      "  515 |   32960 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3743\n",
      "  516 |   33024 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3743\n",
      "  517 |   33088 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3743\n",
      "  518 |   33152 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3743\n",
      "  519 |   33216 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3743\n",
      "  520 |   33280 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  521 |   33344 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  522 |   33408 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  523 |   33472 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  524 |   33536 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  525 |   33600 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  526 |   33664 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  527 |   33728 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  528 |   33792 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  529 |   33856 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  530 |   33920 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  531 |   33984 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  532 |   34048 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  533 |   34112 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  534 |   34176 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  535 |   34240 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  536 |   34304 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  537 |   34368 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  538 |   34432 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  539 |   34496 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  540 |   34560 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  541 |   34624 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  542 |   34688 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  543 |   34752 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  544 |   34816 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  545 |   34880 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3745\n",
      "  546 |   34944 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3748\n",
      "  547 |   35008 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3748\n",
      "  548 |   35072 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3748\n",
      "  549 |   35136 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3748\n",
      "  550 |   35200 |  0.00000E+00 |  0.00000E+00 |      -0.4004 |      -0.3748\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "def save_callback(algorithm):\n",
    "    global iteration\n",
    "    global config\n",
    "\n",
    "    iteration += 1\n",
    "    if iteration % config[\"save_each\"] == 0 or iteration == config[\"generations\"]:\n",
    "        if config[\"problem_args\"][\"n_obj\"] == 1:\n",
    "            sortedpop = sorted(algorithm.pop, key=lambda p: p.F)\n",
    "            X = np.stack([p.X for p in sortedpop])  \n",
    "        else:\n",
    "            X = algorithm.pop.get(\"X\")\n",
    "        \n",
    "        ls = config[\"latent\"](config)\n",
    "        ls.set_from_population(X)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated = algorithm.problem.generator.generate(ls, minibatch=config[\"batch_size\"])\n",
    "            if config[\"task\"] == \"txt2img\":\n",
    "                ext = \"jpg\"\n",
    "            elif config[\"task\"] == \"img2txt\":\n",
    "                ext = \"txt\"\n",
    "            name = \"genetic-it-%d.%s\" % (iteration, ext) if iteration < config[\"generations\"] else \"genetic-it-final.%s\" % (ext, )\n",
    "            algorithm.problem.generator.save(generated, os.path.join(config[\"tmp_folder\"], name))\n",
    "        \n",
    "\n",
    "problem = GenerationProblem(config)\n",
    "operators = get_operators(config)\n",
    "\n",
    "if not os.path.exists(config[\"tmp_folder\"]): os.mkdir(config[\"tmp_folder\"])\n",
    "\n",
    "algorithm = get_algorithm(\n",
    "    config[\"algorithm\"],\n",
    "    pop_size=config[\"pop_size\"],\n",
    "    sampling=operators[\"sampling\"],\n",
    "    crossover=operators[\"crossover\"],\n",
    "    mutation=operators[\"mutation\"],\n",
    "    eliminate_duplicates=True,\n",
    "    callback=save_callback,\n",
    "    **(config[\"algorithm_args\"][config[\"algorithm\"]] if \"algorithm_args\" in config and config[\"algorithm\"] in config[\"algorithm_args\"] else dict())\n",
    ")\n",
    "\n",
    "res = minimize(\n",
    "    problem,\n",
    "    algorithm,\n",
    "    (\"n_gen\", config[\"generations\"]),\n",
    "    save_history=False,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "\n",
    "pickle.dump(dict(\n",
    "    X = res.X,\n",
    "    F = res.F,\n",
    "    G = res.G,\n",
    "    CV = res.CV,\n",
    "), open(os.path.join(config[\"tmp_folder\"], \"genetic_result\"), \"wb\"))\n",
    "\n",
    "if config[\"problem_args\"][\"n_obj\"] == 2:\n",
    "    plot = Scatter(labels=[\"similarity\", \"discriminator\",])\n",
    "    plot.add(res.F, color=\"red\")\n",
    "    plot.save(os.path.join(config[\"tmp_folder\"], \"F.jpg\"))\n",
    "\n",
    "\n",
    "if config[\"problem_args\"][\"n_obj\"] == 1:\n",
    "    sortedpop = sorted(res.pop, key=lambda p: p.F)\n",
    "    X = np.stack([p.X for p in sortedpop])\n",
    "else:\n",
    "    X = res.pop.get(\"X\")\n",
    "\n",
    "ls = config[\"latent\"](config)\n",
    "ls.set_from_population(X)\n",
    "\n",
    "torch.save(ls.state_dict(), os.path.join(config[\"tmp_folder\"], \"ls_result\"))\n",
    "\n",
    "if config[\"problem_args\"][\"n_obj\"] == 1:\n",
    "    X = np.atleast_2d(res.X)\n",
    "else:\n",
    "    try:\n",
    "        result = get_decision_making(\"pseudo-weights\", [0, 1]).do(res.F)\n",
    "    except:\n",
    "        print(\"Warning: cant use pseudo-weights\")\n",
    "        result = get_decomposition(\"asf\").do(res.F, [0, 1]).argmin()\n",
    "\n",
    "    X = res.X[result]\n",
    "    X = np.atleast_2d(X)\n",
    "\n",
    "ls.set_from_population(X)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated = problem.generator.generate(ls)\n",
    "\n",
    "if config[\"task\"] == \"txt2img\":\n",
    "    ext = \"jpg\"\n",
    "elif config[\"task\"] == \"img2txt\":\n",
    "    ext = \"txt\"\n",
    "\n",
    "problem.generator.save(generated, os.path.join(config[\"tmp_folder\"], \"output.%s\" % (ext)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1481f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd75f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd280562",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt7_clone",
   "language": "python",
   "name": "pyt7_clone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
