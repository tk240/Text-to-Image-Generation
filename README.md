# Text-to-Image-Generation
In this project, the task is the classical text-to-image synthesis. We implement
several different architectures to achieve a satisfactory text-to-image model. We use
CLIP model as a text and image encoder. Implemented models are CLIP-GLASS,
VQGAN+CLIP, CLIP Guided Diffusion, Quick CLIP Guided Diffusion, Stable
Diffusion, and Imagen. Approaches, and the overviews of each model is provided in
the report. Furthermore, development process of each model is explained including
the challenges face during the development process. The report ends with the
outputs of the models and their comparisons. Possible future improvements are
also discussed in the conclusion.
