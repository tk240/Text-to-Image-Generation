{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1130f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import math\n",
    "import io\n",
    "import sys\n",
    "\n",
    "from IPython import display\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "sys.path.append('./CLIP')\n",
    "sys.path.append('./guided-diffusion')\n",
    "import clip\n",
    "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e9282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9892df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271a3a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define necessary functions\n",
    "\n",
    "def fetch(url_or_path):\n",
    "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
    "        r = requests.get(url_or_path)\n",
    "        r.raise_for_status()\n",
    "        fd = io.BytesIO()\n",
    "        fd.write(r.content)\n",
    "        fd.seek(0)\n",
    "        return fd\n",
    "    return open(url_or_path, 'rb')\n",
    "\n",
    "\n",
    "def parse_prompt(prompt):\n",
    "    if prompt.startswith('http://') or prompt.startswith('https://'):\n",
    "        vals = prompt.rsplit(':', 2)\n",
    "        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
    "    else:\n",
    "        vals = prompt.rsplit(':', 1)\n",
    "    vals = vals + ['', '1'][len(vals):]\n",
    "    return vals[0], float(vals[1])\n",
    "\n",
    "\n",
    "class MakeCutouts(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow\n",
    "\n",
    "    def forward(self, input):\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        cutouts = []\n",
    "        for _ in range(self.cutn):\n",
    "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
    "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "            offsety = torch.randint(0, sideY - size + 1, ())\n",
    "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
    "        return torch.cat(cutouts)\n",
    "\n",
    "\n",
    "def spherical_dist_loss(x, y):\n",
    "    x = F.normalize(x, dim=-1)\n",
    "    y = F.normalize(y, dim=-1)\n",
    "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
    "\n",
    "\n",
    "def tv_loss(input):\n",
    "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
    "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
    "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
    "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
    "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
    "\n",
    "\n",
    "def range_loss(input):\n",
    "    return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983e6d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define necessary functions\n",
    "\n",
    "def fetch(url_or_path):\n",
    "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
    "        r = requests.get(url_or_path)\n",
    "        r.raise_for_status()\n",
    "        fd = io.BytesIO()\n",
    "        fd.write(r.content)\n",
    "        fd.seek(0)\n",
    "        return fd\n",
    "    return open(url_or_path, 'rb')\n",
    "\n",
    "class MakeCutouts(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow\n",
    "\n",
    "    def forward(self, input):\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        cutouts = []\n",
    "        for _ in range(self.cutn):\n",
    "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
    "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "            offsety = torch.randint(0, sideY - size + 1, ())\n",
    "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
    "        return torch.cat(cutouts)\n",
    "\n",
    "\n",
    "def spherical_dist_loss(x, y):\n",
    "    x = F.normalize(x, dim=-1)\n",
    "    y = F.normalize(y, dim=-1)\n",
    "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
    "\n",
    "\n",
    "def tv_loss(input):\n",
    "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
    "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
    "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
    "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
    "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c73173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings\n",
    "\n",
    "model_config = model_and_diffusion_defaults()\n",
    "model_config.update({\n",
    "    'attention_resolutions': '32, 16, 8',\n",
    "    'class_cond': True,\n",
    "    'diffusion_steps': 1000,\n",
    "    'rescale_timesteps': True,\n",
    "    'timestep_respacing': '1000',  # Modify this value to decrease the number of\n",
    "                                   # timesteps.\n",
    "    'image_size': 512,\n",
    "    'learn_sigma': True,\n",
    "    'noise_schedule': 'linear',\n",
    "    'num_channels': 256,\n",
    "    'num_head_channels': 64,\n",
    "    'num_res_blocks': 2,\n",
    "    'resblock_updown': True,\n",
    "    'use_fp16': True,\n",
    "    'use_scale_shift_norm': True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b82f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "model, diffusion = create_model_and_diffusion(**model_config)\n",
    "model.load_state_dict(torch.load('./models/diff_512.pt', map_location='cpu'))\n",
    "model.requires_grad_(False).eval().to(device)\n",
    "for name, param in model.named_parameters():\n",
    "    if 'qkv' in name or 'norm' in name or 'proj' in name:\n",
    "        param.requires_grad_()\n",
    "if model_config['use_fp16']:\n",
    "    model.convert_to_fp16()\n",
    "\n",
    "clip_model = clip.load('ViT-B/16', jit=False)[0].eval().requires_grad_(False).to(device)\n",
    "check = torch.load('./model_fastai_b16.pth', map_location='cpu')\n",
    "clip_model.load_state_dict(check['model'])\n",
    "clip_size = clip_model.visual.input_resolution\n",
    "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                 std=[0.26862954, 0.26130258, 0.27577711])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144a03b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings\n",
    "\n",
    "model_config = model_and_diffusion_defaults()\n",
    "model_config.update({\n",
    "    'attention_resolutions': '32, 16, 8',\n",
    "    'class_cond': False,\n",
    "    'diffusion_steps': 1000,\n",
    "    'rescale_timesteps': True,\n",
    "    'timestep_respacing': '1000',  # Modify this value to decrease the number of\n",
    "                                   # timesteps.\n",
    "    'image_size': 256,\n",
    "    'learn_sigma': True,\n",
    "    'noise_schedule': 'linear',\n",
    "    'num_channels': 256,\n",
    "    'num_head_channels': 64,\n",
    "    'num_res_blocks': 2,\n",
    "    'resblock_updown': True,\n",
    "    'use_checkpoint': False,\n",
    "    'use_fp16': True,\n",
    "    'use_scale_shift_norm': True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061831a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "model, diffusion = create_model_and_diffusion(**model_config)\n",
    "model.load_state_dict(torch.load('./models/diff_256.pt', map_location='cpu'))\n",
    "model.requires_grad_(False).eval().to(device)\n",
    "if model_config['use_fp16']:\n",
    "    model.convert_to_fp16()\n",
    "\n",
    "clip_model = clip.load('ViT-B/16', jit=False)[0].eval().requires_grad_(False).to(device)\n",
    "check = torch.load('./model_fastai_b16.pth', map_location='cpu')\n",
    "clip_model.load_state_dict(check['model'])\n",
    "clip_size = clip_model.visual.input_resolution\n",
    "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                 std=[0.26862954, 0.26130258, 0.27577711])\n",
    "lpips_model = lpips.LPIPS(net='vgg').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509d726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = 'A dog in front of sea'\n",
    "batch_size = 1\n",
    "clip_guidance_scale = 1000  # Controls how much the image should look like the prompt.\n",
    "tv_scale = 150              # Controls the smoothness of the final output.\n",
    "cutn = 40\n",
    "cut_pow = 0.5\n",
    "n_batches = 1\n",
    "init_image = None   # This can be an URL or local path and must be in quotes.\n",
    "skip_timesteps = 0  # This needs to be between approx. 200 and 500 when using an init image.\n",
    "                    # Higher values make the output look more like the init.\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a4b89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ['A giraffe among zebras']\n",
    "image_prompts = []\n",
    "batch_size = 1\n",
    "clip_guidance_scale = 1000  # Controls how much the image should look like the prompt.\n",
    "tv_scale = 150              # Controls the smoothness of the final output.\n",
    "range_scale = 50            # Controls how far out of range RGB values are allowed to be.\n",
    "cutn = 16\n",
    "n_batches = 1\n",
    "init_image = None   # This can be an URL or local path and must be in quotes.\n",
    "skip_timesteps = 0  # This needs to be between approx. 200 and 500 when using an init image.\n",
    "                    # Higher values make the output look more like the init.\n",
    "init_scale = 0      # This enhances the effect of the init image, a good value is 1000.\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c02b9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def do_run():\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    make_cutouts = MakeCutouts(clip_size, cutn)\n",
    "    side_x = side_y = model_config['image_size']\n",
    "\n",
    "    target_embeds, weights = [], []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        txt, weight = parse_prompt(prompt)\n",
    "        target_embeds.append(clip_model.encode_text(clip.tokenize(txt).to(device)).float())\n",
    "        weights.append(weight)\n",
    "\n",
    "    for prompt in image_prompts:\n",
    "        path, weight = parse_prompt(prompt)\n",
    "        img = Image.open(fetch(path)).convert('RGB')\n",
    "        img = TF.resize(img, min(side_x, side_y, *img.size), transforms.InterpolationMode.LANCZOS)\n",
    "        batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
    "        embed = clip_model.encode_image(normalize(batch)).float()\n",
    "        target_embeds.append(embed)\n",
    "        weights.extend([weight / cutn] * cutn)\n",
    "\n",
    "    target_embeds = torch.cat(target_embeds)\n",
    "    weights = torch.tensor(weights, device=device)\n",
    "    if weights.sum().abs() < 1e-3:\n",
    "        raise RuntimeError('The weights must not sum to 0.')\n",
    "    weights /= weights.sum().abs()\n",
    "\n",
    "    init = None\n",
    "    if init_image is not None:\n",
    "        init = Image.open(fetch(init_image)).convert('RGB')\n",
    "        init = init.resize((side_x, side_y), Image.LANCZOS)\n",
    "        init = TF.to_tensor(init).to(device).unsqueeze(0).mul(2).sub(1)\n",
    "\n",
    "    cur_t = None\n",
    "\n",
    "    def cond_fn(x, t, out, y=None):\n",
    "        n = x.shape[0]\n",
    "        fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
    "        x_in = out['pred_xstart'] * fac + x * (1 - fac)\n",
    "        clip_in = normalize(make_cutouts(x_in.add(1).div(2)))\n",
    "        image_embeds = clip_model.encode_image(clip_in).float()\n",
    "        dists = spherical_dist_loss(image_embeds.unsqueeze(1), target_embeds.unsqueeze(0))\n",
    "        dists = dists.view([cutn, n, -1])\n",
    "        losses = dists.mul(weights).sum(2).mean(0)\n",
    "        tv_losses = tv_loss(x_in)\n",
    "        range_losses = range_loss(out['pred_xstart'])\n",
    "        loss = losses.sum() * clip_guidance_scale + tv_losses.sum() * tv_scale + range_losses.sum() * range_scale\n",
    "        if init is not None and init_scale:\n",
    "            init_losses = lpips_model(x_in, init)\n",
    "            loss = loss + init_losses.sum() * init_scale\n",
    "        return -torch.autograd.grad(loss, x)[0]\n",
    "\n",
    "    if model_config['timestep_respacing'].startswith('ddim'):\n",
    "        sample_fn = diffusion.ddim_sample_loop_progressive\n",
    "    else:\n",
    "        sample_fn = diffusion.p_sample_loop_progressive\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        cur_t = diffusion.num_timesteps - skip_timesteps - 1\n",
    "\n",
    "        samples = sample_fn(\n",
    "            model,\n",
    "            (batch_size, 3, side_y, side_x),\n",
    "            clip_denoised=False,\n",
    "            model_kwargs={},\n",
    "            cond_fn=cond_fn,\n",
    "            progress=True,\n",
    "            skip_timesteps=skip_timesteps,\n",
    "            init_image=init,\n",
    "            randomize_class=True,\n",
    "            cond_fn_with_grad=True,\n",
    "        )\n",
    "\n",
    "        for j, sample in enumerate(samples):\n",
    "            cur_t -= 1\n",
    "            if j % 100 == 0 or cur_t == -1:\n",
    "                print()\n",
    "                for k, image in enumerate(sample['pred_xstart']):\n",
    "                    filename = f'progress_{i * batch_size + k:05}.png'\n",
    "                    TF.to_pil_image(image.add(1).div(2).clamp(0, 1)).save(filename)\n",
    "                    tqdm.write(f'Batch {i}, step {j}, output {k}:')\n",
    "                    display.display(display.Image(filename))\n",
    "\n",
    "gc.collect()\n",
    "do_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988d24eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6dcdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_run():\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    text_embed = clip_model.encode_text(clip.tokenize(prompt).to(device)).float()\n",
    "\n",
    "    init = None\n",
    "    if init_image is not None:\n",
    "        init = Image.open(fetch(init_image)).convert('RGB')\n",
    "        init = init.resize((model_config['image_size'], model_config['image_size']), Image.LANCZOS)\n",
    "        init = TF.to_tensor(init).to(device).unsqueeze(0).mul(2).sub(1)\n",
    "\n",
    "    make_cutouts = MakeCutouts(clip_size, cutn, cut_pow)\n",
    "\n",
    "    cur_t = None\n",
    "\n",
    "    def cond_fn(x, t, y=None):\n",
    "        with torch.enable_grad():\n",
    "            x = x.detach().requires_grad_()\n",
    "            n = x.shape[0]\n",
    "            my_t = torch.ones([n], device=device, dtype=torch.long) * cur_t\n",
    "            out = diffusion.p_mean_variance(model, x, my_t, clip_denoised=False, model_kwargs={'y': y})\n",
    "            fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
    "            x_in = out['pred_xstart'] * fac + x * (1 - fac)\n",
    "            clip_in = normalize(make_cutouts(x_in.add(1).div(2)))\n",
    "            image_embeds = clip_model.encode_image(clip_in).float().view([cutn, n, -1])\n",
    "            dists = spherical_dist_loss(image_embeds, text_embed.unsqueeze(0))\n",
    "            losses = dists.mean(0)\n",
    "            tv_losses = tv_loss(x_in)\n",
    "            loss = losses.sum() * clip_guidance_scale + tv_losses.sum() * tv_scale\n",
    "            return -torch.autograd.grad(loss, x)[0]\n",
    "\n",
    "    if model_config['timestep_respacing'].startswith('ddim'):\n",
    "        sample_fn = diffusion.ddim_sample_loop_progressive\n",
    "    else:\n",
    "        sample_fn = diffusion.p_sample_loop_progressive\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        cur_t = diffusion.num_timesteps - skip_timesteps - 1\n",
    "\n",
    "        samples = sample_fn(\n",
    "            model,\n",
    "            (batch_size, 3, model_config['image_size'], model_config['image_size']),\n",
    "            clip_denoised=False,\n",
    "            model_kwargs={'y': torch.zeros([batch_size], device=device, dtype=torch.long)},\n",
    "            cond_fn=cond_fn,\n",
    "            progress=True,\n",
    "            skip_timesteps=skip_timesteps,\n",
    "            init_image=init,\n",
    "            randomize_class=True,\n",
    "        )\n",
    "\n",
    "        for j, sample in enumerate(samples):\n",
    "            cur_t -= 1\n",
    "            if j % 100 == 0 or cur_t == -1:\n",
    "                print()\n",
    "                for k, image in enumerate(sample['pred_xstart']):\n",
    "                    filename = f'progress_{i * batch_size + k:05}.png'\n",
    "                    TF.to_pil_image(image.add(1).div(2).clamp(0, 1)).save(filename)\n",
    "                    tqdm.write(f'Batch {i}, step {j}, output {k}:')\n",
    "                    display.display(display.Image(filename))\n",
    "\n",
    "do_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7951db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addcbb98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dalle_n",
   "language": "python",
   "name": "dalle_n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
